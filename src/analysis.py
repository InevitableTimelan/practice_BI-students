# -*- coding: utf-8 -*-
"""
BIå­¦ç”Ÿæ•°æ®åˆ†æè„šæœ¬
å›ç­”å››ä¸ªæ ¸å¿ƒä¸šåŠ¡é—®é¢˜:
1. Admissions Optimization
Should entry exams remain the primary admissions filter?

Your task is to evaluate the predictive power of entry exam scores compared to other features such as prior education, age, gender, and study hours.

âœ… Deliverables:

Feature importance ranking for predicting Python and DB scores
Admission policy recommendation (e.g., retain exams, add screening tools, adjust thresholds)
Business rationale and risk analysis

# éœ€è¦åˆ†æçš„å†…å®¹ï¼š
# 1. å…¥å­¦è€ƒè¯•ä¸æˆç»©çš„ç›¸å…³æ€§ï¼ˆç›¸å…³ç³»æ•°çŸ©é˜µï¼‰
# 2. å…¥å­¦è€ƒè¯•çš„é¢„æµ‹èƒ½åŠ›ï¼ˆçº¿æ€§å›å½’/RÂ²åˆ†æ•°ï¼‰
# 3. ç‰¹å¾é‡è¦æ€§æ’åï¼ˆéšæœºæ£®æ—/ç‰¹å¾é‡è¦æ€§ï¼‰
# 4. ä¸å…¶ä»–ç‰¹å¾ï¼ˆæ•™è‚²èƒŒæ™¯ã€å¹´é¾„ã€å­¦ä¹ æ—¶é•¿ï¼‰çš„æ¯”è¾ƒ

# æœºå™¨å­¦ä¹ åº”ç”¨ï¼šçº¿æ€§å›å½’ã€éšæœºæ£®æ—å›å½’
# è¯„ä¼°æŒ‡æ ‡ï¼šRÂ²åˆ†æ•°ã€MSEã€ç‰¹å¾é‡è¦æ€§

2. Curriculum Support Strategy
Are there at-risk student groups who need extra support?

Your task is to uncover whether certain backgrounds (e.g., prior education level, country, residence type) correlate with poor performance and recommend targeted interventions.

âœ… Deliverables:

At-risk segment identification
Support program design (e.g., prep course, mentoring)
Expected outcomes, costs, and KPIs

# éœ€è¦åˆ†æçš„å†…å®¹ï¼š
# 1. è¯†åˆ«é«˜é£é™©å­¦ç”Ÿï¼ˆå¹³å‡åˆ†<60åˆ†ï¼‰
# 2. é«˜é£é™©å­¦ç”Ÿçš„ç‰¹å¾åˆ†æï¼ˆåˆ†ç»„ç»Ÿè®¡ï¼‰
# 3. å“ªäº›èƒŒæ™¯å› ç´ ä¸ä½æˆç»©ç›¸å…³ï¼ˆå¡æ–¹æ£€éªŒ/ç›¸å…³æ€§åˆ†æï¼‰

# æœºå™¨å­¦ä¹ åº”ç”¨ï¼šé€»è¾‘å›å½’åˆ†ç±»ã€å†³ç­–æ ‘
# è¯„ä¼°æŒ‡æ ‡ï¼šå‡†ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°

3. Resource Allocation & Program ROI

How can we allocate resources for maximum student success?

Your task is to segment students by success profiles and suggest differentiated teaching/facility strategies.

âœ… Deliverables:

Performance drivers
Student segmentation
Resource allocation plan and ROI projection      

# éœ€è¦åˆ†æçš„å†…å®¹ï¼š
# 1. å­¦ç”Ÿç»†åˆ†ï¼ˆèšç±»åˆ†æï¼‰
# 2. ä¸åŒç¾¤ä½“çš„ç‰¹å¾åˆ†æ
# 3. ROIè®¡ç®—æ¨¡å‹ï¼ˆæˆæœ¬æ•ˆç›Šåˆ†æï¼‰

# æœºå™¨å­¦ä¹ åº”ç”¨ï¼šK-meansèšç±»
# è¯„ä¼°æŒ‡æ ‡ï¼šè½®å»“ç³»æ•°ã€èšç±»è´¨é‡
 
Bonus Challenge

â€œIf you could implement only one intervention to improve student outcomes, what would it be â€” and why?â€

# ç»¼åˆä»¥ä¸Šåˆ†æï¼Œæå‡ºä¸€ä¸ªæœ€æœ‰æ•ˆçš„å¹²é¢„æªæ–½
# ç”¨æ•°æ®æ”¯æŒé€‰æ‹©
# æä¾›ROIé¢„æµ‹

2026å¹´2æœˆ
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier
from sklearn.metrics import r2_score,mean_squared_error,accuracy_score,classification_report
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler,LabelEncoder
import warnings
warnings.filterwarnings("ignore")

#è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾å½¢æ ·å¼

plt.rcParams['font.sans-serif'] = ['SimHei','Microsoft YaHei','Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")
sns.set_palette("husl")

#é¢œè‰²æ–¹æ¡ˆ

COLORS = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']

#è¯»å–æ•°æ®

def load_data(filepath='../data/processed/cleaned_student_data.csv'):
    """è¯»å–æœ¬åœ°æ–‡ä»¶"""
    try:
        df = pd.read_csv(filepath,encoding='gbk')
        print(f"æˆåŠŸè¯»å–æ•°æ®ï¼š{df.shape[0]}è¡Œï¼Œ{df.shape[1]}åˆ—")
        return df
    except Exception as e:
        print(f"è¯»å–å¤±è´¥ï¼š{e}")
        return None

#æ•°æ®åˆ†æ

def exploration_data_analysis(df):
    """åŸºç¡€æ€§æ•°æ®åˆ†æ"""
    print("\n" + "="*60)
    print("æ¢ç´¢æ€§æ•°æ®åˆ†æ (EDA)")
    print("="*60)
    
    #æ•°æ®æ¦‚å†µ
    
    print(f"æ•°æ®å½¢çŠ¶ï¼š{df.shape}")
    print(f"\nåˆ—å: {list(df.columns)}")
    print(f"\næ•°æ®ç±»å‹:\n{df.dtypes}")
    
    #æè¿°æ€§ç»Ÿè®¡
    
    print("æè¿°æ€§ç»Ÿè®¡ï¼š")
    print(df.describe().round(2))
    
    #ç¼ºå¤±å€¼æ£€æŸ¥
    
    missing_values = df.isnull().sum()
    if missing_values.sum() > 0:
       print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
       missing_df = missing_values[missing_values > 0]
       for col, count in missing_df.items():
           percentage = (count / len(df)) * 100
           print(f"  {col}: {count} ä¸ª ({percentage:.1f}%)")
    else:
       print("âœ… æ— ç¼ºå¤±å€¼")
    
    #å…³é”®æŒ‡æ ‡åˆ†å¸ƒ
    
    print("å…³é”®æŒ‡æ ‡åˆ†å¸ƒï¼š")
    key = ['python','db','total_score','average_score','studyhours','entryexam']
    for col in key:
        print(f"{col}: å‡å€¼={df[col].mean():.1f}, æ ‡å‡†å·®={df[col].std():.1f}, "f"æœ€å°å€¼={df[col].min():.1f}, æœ€å¤§å€¼={df[col].max():.1f}")
    
    return df

# ====== é—®é¢˜1ï¼šæ‹›ç”Ÿä¼˜åŒ– ======

def missions_recruit(df):
    """
    é—®é¢˜1: Should entry exams remain the primary admissions filter?
    åˆ†æå…¥å­¦è€ƒè¯•ä½œä¸ºä¸»è¦å½•å–ç­›é€‰å·¥å…·çš„åˆç†æ€§
    """
    
    print("\n" + "="*60)
    print("é—®é¢˜1: æ‹›ç”Ÿä¼˜åŒ–åˆ†æ")
    print("="*60)
    
    results = {}
    
    #1.ç›¸å…³ç³»æ•°åˆ†æ
    
    print("\nç›¸å…³ç³»æ•°åˆ†æ (å…¥å­¦è€ƒè¯•ä¸æˆç»©çš„å…³ç³»):")
    
    corr_features = ['entryexam', 'python', 'db', 'average_score', 
                     'studyhours', 'age']
    available_features = [col for col in corr_features if col in df.columns]
    
    if len(available_features) >= 2:
        corr_matrix = df[available_features].corr()
        print("\nç›¸å…³ç³»æ•°çŸ©é˜µï¼š")
        print(corr_matrix.round(3))
    
    #å¯è§†åŒ–ç›¸å…³ç³»æ•°çƒ­å›¾
    
    plt.figure(figsize=(10,8))
    sns.heatmap(corr_matrix,annot=True,cmap='coolwarm',center=0,fmt='.2f',square=True,linewidths=0.5)
    plt.title("ç‰¹å¾ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾")
    plt.tight_layout()
    plt.savefig('../visualizations/correlation_heatmap.png', dpi=300, bbox_inches='tight')
    print("ç›¸å…³ç³»æ•°çƒ­å›¾å·²ä¿å­˜")
    
    #2.å…¥å­¦è€ƒè¯•ä¸å¹³å‡æˆç»©çš„æ•£ç‚¹å›¾
    
    if all(col in df.columns for col in ['entryexam', 'average_score']):
        plt.figure(figsize=(12,5))
        
        plt.subplot(1, 2, 1)
        plt.scatter(df['entryexam'], df['average_score'], alpha=0.6, color=COLORS[0])
        plt.xlabel('å…¥å­¦æˆç»©')
        plt.ylabel('å¹³å‡æˆç»©')
        plt.title('å…¥å­¦è€ƒè¯• vs å¹³å‡æˆç»©')
        
        #æ·»åŠ å›å½’çº¿
        
        z = np.polyfit(df['entryexam'], df['average_score'], 1)
        p = np.poly1d(z)
        plt.plot(df['entryexam'], p(df['entryexam']), "r--", alpha=0.8)
        
        #è®¡ç®—RÂ²
        
        r2 = r2_score(df['average_score'], p(df['entryexam']))
        plt.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')
        
        plt.subplot(1, 2, 2)
        
        # å…¥å­¦è€ƒè¯•æˆç»©åˆ†å¸ƒ
        
        plt.hist(df['entryexam'], bins=20, color=COLORS[1], edgecolor='black', alpha=0.7)
        plt.xlabel('å…¥å­¦è€ƒè¯•æˆç»©')
        plt.ylabel('é¢‘æ¬¡')
        plt.title('å…¥å­¦è€ƒè¯•æˆç»©åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.savefig('../visualizations/entry_exam_analysis.png', dpi=300, bbox_inches='tight')
        print("\å…¥å­¦è€ƒè¯•åˆ†æå›¾è¡¨å·²ä¿å­˜")
        
        results['entryexam_r2'] = r2
        
    #3.ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆä½¿ç”¨éšæœºæ£®æ—ï¼‰
    
    print("\nç‰¹å¾é‡è¦æ€§åˆ†æ (é¢„æµ‹å¹³å‡æˆç»©):")
    
    #å‡†å¤‡ç‰¹å¾
    
    feature_cols = []
   
    if 'entryexam' in df.columns:
       feature_cols.append('entryexam')
    if 'age' in df.columns:
       feature_cols.append('age')
    if 'studyhours' in df.columns:
       feature_cols.append('studyhours')
    
    #æ·»åŠ ç¼–ç åçš„åˆ†ç±»ç‰¹å¾
    
    categorical_cols = ['gender', 'preveducation', 'country', 'residence']
    for col in categorical_cols:
        if col in df.columns:
            # ä½¿ç”¨æ ‡ç­¾ç¼–ç 
            le = LabelEncoder()
            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
            feature_cols.append(f'{col}_encoded')
    
    if len(feature_cols) >= 2 and 'average_score' in df.columns:
        X = df[feature_cols]
        y = df['average_score']
        
        # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
        
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        # ç‰¹å¾é‡è¦æ€§
        
        importance_df = pd.DataFrame({'feature': feature_cols,'importance': rf.feature_importances_ }).sort_values('importance', ascending=False)
        
        print("\nç‰¹å¾é‡è¦æ€§æ’å:")
        print(importance_df)
        
        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        
        plt.figure(figsize=(10, 6))
        bars = plt.barh(range(len(importance_df)), importance_df['importance'])
        plt.yticks(range(len(importance_df)), importance_df['feature'])
        plt.xlabel('ç‰¹å¾é‡è¦æ€§')
        plt.title('å½±å“å­¦ç”Ÿæˆç»©çš„ç‰¹å¾é‡è¦æ€§')
        plt.gca().invert_yaxis()
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        
        for i, (bar, importance) in enumerate(zip(bars, importance_df['importance'])):
            plt.text(importance + 0.01, bar.get_y() + bar.get_height()/2, 
                    f'{importance:.3f}', ha='left', va='center')
        
        plt.tight_layout()
        plt.savefig('../visualizations/feature_importance.png', dpi=300, bbox_inches='tight')
        print("âœ… ç‰¹å¾é‡è¦æ€§å›¾è¡¨å·²ä¿å­˜")
        
        results['feature_importance'] = importance_df
        
    #4.å¤šå˜é‡å›å½’åˆ†æ
    
    print("\nå¤šå˜é‡å›å½’åˆ†æ:")
    if len(feature_cols) >= 2 and 'average_score' in df.columns:
        
       # è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹
       
       lr = LinearRegression()
       lr.fit(X, y)
       y_pred = lr.predict(X)
       
       # è¯„ä¼°æŒ‡æ ‡
       
       r2 = r2_score(y, y_pred)
       mse = mean_squared_error(y, y_pred)
       
       print("å¤šå˜é‡å›å½’æ¨¡å‹æ€§èƒ½:")
       print(f"RÂ²åˆ†æ•°: {r2:.3f}")
       print(f"å‡æ–¹è¯¯å·®: {mse:.3f}")
       
       # ç³»æ•°åˆ†æ
       
       coefficients = pd.DataFrame({'feature': ['intercept'] + feature_cols,'coefficient': [lr.intercept_] + list(lr.coef_) })
       
       print("\nå›å½’ç³»æ•°:")
       print(coefficients.round(3))
       
       results['regression_r2'] = r2
       results['regression_coefficients'] = coefficients
       
    # 5.æ‹›ç”Ÿæ”¿ç­–å»ºè®®
    
    print("\n æ‹›ç”Ÿæ”¿ç­–å»ºè®®:")
    
    if 'feature_importance' in results:
        top_feature = results['feature_importance'].iloc[0]['feature']
        top_importance = results['feature_importance'].iloc[0]['importance']
        
        print(f"1.ä¸»è¦å‘ç°:æœ€é‡è¦çš„é¢„æµ‹ç‰¹å¾æ˜¯ '{top_feature}' (é‡è¦æ€§: {top_importance:.3f})")
        
    if 'entryexam_r2' in results:
        entry_r2 = results['entryexam_r2']
        if entry_r2 > 0.5:
            print("2.å…¥å­¦è€ƒè¯•æœ‰æ•ˆæ€§:å…¥å­¦è€ƒè¯•æˆç»©æ˜¯æˆç»©çš„å¼ºé¢„æµ‹æŒ‡æ ‡ (RÂ² > 0.5)")
            print("å»ºè®®:ç»§ç»­å°†å…¥å­¦è€ƒè¯•ä½œä¸ºä¸»è¦ç­›é€‰å·¥å…·")
        elif entry_r2 > 0.3:
            print("2.å…¥å­¦è€ƒè¯•æœ‰æ•ˆæ€§:å…¥å­¦è€ƒè¯•æˆç»©æ˜¯æˆç»©çš„ä¸­ç­‰é¢„æµ‹æŒ‡æ ‡ (0.3 < RÂ² < 0.5)")
            print("å»ºè®®:å°†å…¥å­¦è€ƒè¯•ä¸å…¶ä»–å› ç´ ç»“åˆä½¿ç”¨")
        else:
            print("2.å…¥å­¦è€ƒè¯•æœ‰æ•ˆæ€§:å…¥å­¦è€ƒè¯•æˆç»©çš„é¢„æµ‹èƒ½åŠ›è¾ƒå¼± (RÂ² < 0.3)")
            print("å»ºè®®:è€ƒè™‘è¡¥å……å…¶ä»–ç­›é€‰å·¥å…·")
    
    print("3.æ¨èæªæ–½:")
    print(" -å°†å…¥å­¦è€ƒè¯•ä¸æ•™è‚²èƒŒæ™¯ã€å­¦ä¹ æ—¶é•¿ç­‰å› ç´ ç»“åˆè¯„ä¼°")
    print(" -ä¸ºä¸åŒèƒŒæ™¯çš„å­¦ç”Ÿè®¾å®šå·®å¼‚åŒ–çš„å½•å–æ ‡å‡†")
    print(" -å»ºç«‹é¢„æµ‹æ¨¡å‹ï¼Œæå‰è¯†åˆ«æœ‰æ½œåŠ›çš„å­¦ç”Ÿ")
    
    return results

# ====== é—®é¢˜2: è¯¾ç¨‹æ”¯æŒç­–ç•¥ ======

def missions_support(df):
    """
    é—®é¢˜2: Are there at-risk student groups who need extra support?
    è¯†åˆ«éœ€è¦é¢å¤–æ”¯æŒçš„é«˜é£é™©å­¦ç”Ÿç¾¤ä½“
    """
    
    print("\n" + "="*60)
    print("é—®é¢˜2: è¯¾ç¨‹æ”¯æŒç­–ç•¥åˆ†æ")
    print("="*60)
    
    results = {}
    
    #1.å®šä¹‰é«˜é£é™©å­¦ç”Ÿ (å¹³å‡åˆ† < 60)
    
    if 'average_score' in df.columns:
       df['is_at_risk'] = df['average_score'] < 60
       at_risk_count = df['is_at_risk'].sum()
       at_risk_percentage = at_risk_count / len(df) * 100
    
    print("\né«˜é£é™©å­¦ç”Ÿç»Ÿè®¡ï¼š")
    print(f"é«˜é£é™©å­¦ç”Ÿäººæ•°ï¼š{at_risk_count}äºº")
    print(f"é«˜é£é™©äººæ•°ç™¾åˆ†æ¯”ï¼š{at_risk_percentage:.1f}%")
    
    results['at_risk_count'] = at_risk_count
    results['at_risk_percentage'] = at_risk_percentage
    
    #2.é«˜é£é™©å­¦ç”Ÿç‰¹å¾åˆ†æ
    
    print("\né«˜é£é™©å­¦ç”Ÿç‰¹å¾åˆ†æï¼š")
    
    #æŒ‰æ€§åˆ«åˆ†æ
    
    if 'gender' in df.columns and 'is_at_risk' in df.columns:
        gender_risk = df.groupby('gender')['is_at_risk'].mean()*100
        print("\næŒ‰æ€§åˆ«åˆ†å¸ƒçš„é£é™©æ¯”ä¾‹ï¼š")
        for gender, risk in gender_risk.items():
            print(f"{gender}: {risk:.1f}%")
        
    #æŒ‰æ•™è‚²èƒŒæ™¯åˆ†æ
    
    if 'preveducation' in df.columns and 'is_at_risk' in df.columns:
        preveducation_risk = df.groupby('preveducation')['is_at_risk'].mean()*100
        print("\næŒ‰æ•™è‚²èƒŒæ™¯åˆ†å¸ƒçš„é£é™©æ¯”ä¾‹ï¼š")
        for preveducation, risk in preveducation_risk.items():
            print(f"{preveducation}:{risk:.1f}%")
    
    #æŒ‰å±…ä½åœ°åˆ†æ
    
    if 'residence' in df.columns and 'is_at_risk' in df.columns:
        residence_risk = df.groupby('residence')['is_at_risk'].mean()*100
        print("\næŒ‰å±…ä½åœ°åˆ†å¸ƒçš„é£é™©æ¯”ä¾‹ï¼š")
        for residence, risk in residence_risk.items():
            print(f"{residence}:{risk:.1f}%")
    
    # 3. é«˜é£é™©å­¦ç”Ÿé¢„æµ‹æ¨¡å‹
    
    print("\né«˜é£é™©å­¦ç”Ÿé¢„æµ‹æ¨¡å‹:")
    
    # å‡†å¤‡ç‰¹å¾
    
    feature_cols = []
    if 'entryexam' in df.columns:
       feature_cols.append('entryexam')
    if 'age' in df.columns:
       feature_cols.append('age')
    if 'studyhours' in df.columns:
       feature_cols.append('studyhours')
    
    categorical_col = ['gender','preveducation','country','residence']
    for col in categorical_col:
        if col in df.columns:
            le = LabelEncoder()
            df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))
            feature_cols.append(f'{col}_encoded')
    
    if len(feature_cols) >= 2 and 'is_at_risk' in df.columns:
        X = df[feature_cols]
        y = df['is_at_risk'].astype(int)
        
        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)
        
        lr_clf = LogisticRegression(max_iter=1000,random_state=42)
        lr_clf.fit(X_train,y_train)
        y_pred = lr_clf.predict(X_test)
        
        accuracy = accuracy_score(y_test, y_pred)
        print(f"é€»è¾‘å›å½’æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.3f}")
        print("\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred))
        
        rf_clf = RandomForestClassifier(n_estimators=100,random_state=42)
        rf_clf.fit(X_train,y_train)
        y_pred_rf = rf_clf.predict(X_test)
        accuracy_rf = accuracy_score(y_test, y_pred_rf)
        
        print(f"\néšæœºæ£®æ—æ¨¡å‹å‡†ç¡®ç‡: {accuracy_rf:.3f}")
        print("éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§:")
        rf_importance = pd.DataFrame({'feature': feature_cols,'importance': rf_clf.feature_importances_}).sort_values('importance', ascending=False)
        print(rf_importance.round(3))
        
        results['lr_accuracy'] = accuracy
        results['rf_accuracy'] = accuracy_rf
        results['rf_feature_importance'] = rf_importance
        
    #4.æ”¯æŒè®¡åˆ’è®¾è®¡
    print("\nğŸ’¡ æ”¯æŒè®¡åˆ’è®¾è®¡:")
    print("1. ç›®æ ‡ç¾¤ä½“: å¹³å‡æˆç»©ä½äº60åˆ†çš„å­¦ç”Ÿ")
    print("2. æ”¯æŒæªæ–½:")
    print("   - ä¸ªæ€§åŒ–è¾…å¯¼è¯¾ç¨‹ (æ¯å‘¨2å°æ—¶)")
    print("   - å­¦ä¹ æŠ€å·§å·¥ä½œåŠ")
    print("   - åŒä¼´å¯¼å¸ˆè®¡åˆ’")
    print("   - é¢å¤–å­¦ä¹ èµ„æºæä¾›")
    print("3. é¢„æœŸæˆæœ:")
    print("   - é«˜é£é™©å­¦ç”Ÿæˆç»©æå‡20%")
    print("   - é«˜é£é™©å­¦ç”Ÿæ¯”ä¾‹å‡å°‘30%")
    print("4. å…³é”®ç»©æ•ˆæŒ‡æ ‡ (KPIs):")
    print("   - é«˜é£é™©å­¦ç”Ÿå¹³å‡æˆç»©æå‡")
    print("   - é«˜é£é™©å­¦ç”Ÿæ•°é‡å‡å°‘")
    print("   - å­¦ç”Ÿæ»¡æ„åº¦è°ƒæŸ¥å¾—åˆ†")
    
    return results

# ====== é—®é¢˜3: èµ„æºåˆ†é…ä¸ROI ======

def missions_allocation(df):
    """
    é—®é¢˜3: Resource Allocation & Program ROI
    å­¦ç”Ÿç»†åˆ†ä¸èµ„æºåˆ†é…ç­–ç•¥
    """
    
    print("\n" + "="*60)
    print("é—®é¢˜3:èµ„æºåˆ†é…ä¸ROIåˆ†æ")
    print("="*60)
    
    results = {}
    
    #1.å­¦ç”Ÿç»†åˆ†(èšç±»åˆ†æ)
    
    print("\nå­¦ç”Ÿç»†åˆ†åˆ†æ (èšç±»):")
    
    #é€‰æ‹©èšç±»ç‰¹å¾
    
    cluster_features = []
    if 'average_score' in df.columns:
        cluster_features.append('average_score')
    if 'entryexam' in df.columns:
        cluster_features.append('entryexam')
    if 'studyhours' in df.columns:
        cluster_features.append('studyhours')
        
    if len(cluster_features) >= 2:
        
        #æ ‡å‡†åŒ–ç‰¹å¾
        
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(df[cluster_features])
       
       #ä½¿ç”¨è‚˜éƒ¨æ³•åˆ™ç¡®å®šæœ€ä½³Kå€¼
       
        inertias = []
        K_range = range(1, 6)
        for k in K_range:
            kmeans = KMeans(n_clusters=k,random_state=42,n_init=10)
            kmeans.fit(X_scaled)
            inertias.append(kmeans.inertia_)
        
        #å¯è§†åŒ–è‚˜éƒ¨æ³•åˆ™
        
        plt.figure(figsize=(10, 6))
        plt.plot(K_range, inertias, 'bo-')
        plt.xlabel('èšç±»æ•°é‡ (K)')
        plt.ylabel('å†…èšåŠ› (Inertia)')
        plt.title('è‚˜éƒ¨æ³•åˆ™: ç¡®å®šæœ€ä½³èšç±»æ•°é‡')
        plt.xticks(K_range)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig('../visualizations/elbow_method.png', dpi=300, bbox_inches='tight')
        
        #é€‰æ‹©K=3 
        
        optimal_k = 3
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        df['cluster'] = kmeans.fit_predict(X_scaled)
        
        #åˆ†ææ¯ä¸ªèšç±»
        
        cluster_analysis = df.groupby('cluster').agg({'average_score': 'mean','entryexam': 'mean','studyhours': 'mean','risk_level': lambda x: (x == 'high_risk').mean() * 100}).round(2)
        
        print(f"\nå°†å­¦ç”Ÿåˆ†ä¸º{optimal_k}ä¸ªç¾¤ä½“:")
        print(cluster_analysis)
        
        cluster_names = {0: 'è¡¨ç°ä¼˜ç§€å­¦ç”Ÿ',1: 'ä¸­ç­‰è¡¨ç°å­¦ç”Ÿ', 2: 'éœ€è¦æ”¯æŒå­¦ç”Ÿ'}
        df['segment'] = df['cluster'].map(cluster_names)
        
        #å¯è§†åŒ–èšç±»ç»“æœ
        
        if len(cluster_features) >= 2:
            plt.figure(figsize=(10, 8))
            
            #é€‰æ‹©å‰ä¸¤ä¸ªç‰¹å¾è¿›è¡Œå¯è§†åŒ–
            
            x_feature, y_feature = cluster_features[0], cluster_features[1]
            
            scatter = plt.scatter(df[x_feature], df[y_feature], c=df['cluster'], cmap='viridis', alpha=0.7)
            plt.xlabel(x_feature)
            plt.ylabel(y_feature)
            plt.title('å­¦ç”Ÿèšç±»å¯è§†åŒ–')
            plt.colorbar(scatter, label='èšç±»')
            
            #æ ‡è®°èšç±»ä¸­å¿ƒ
            
            centers = scaler.inverse_transform(kmeans.cluster_centers_)
            plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.8, marker='X')
            
            plt.tight_layout()
            plt.savefig('../visualizations/cluster_analysis.png', dpi=300, bbox_inches='tight')
            print("èšç±»åˆ†æå›¾è¡¨å·²ä¿å­˜")
        
        results['clusters'] = cluster_analysis
        results['cluster_centers'] = centers
        
    #2.èµ„æºåˆ†é…ç­–ç•¥
        
    print("\nèµ„æºåˆ†é…ç­–ç•¥:")
   
    if 'segment' in df.columns:
        segment_counts = df['segment'].value_counts()
       
        print("åŸºäºå­¦ç”Ÿç»†åˆ†çš„ç»“æœåˆ†é…:")
        for segment, count in segment_counts.items():
            percentage = count / len(df) * 100
            print(f"\n{segment} ({count}äºº, {percentage:.1f}%):")
           
            if 'éœ€è¦æ”¯æŒ' in segment:
                print(" - èµ„æºåˆ†é…: é«˜æŠ•å…¥ (40%æ€»èµ„æº)")
                print(" - æ”¯æŒæªæ–½: ä¸ªæ€§åŒ–è¾…å¯¼ã€é¢å¤–ç»ƒä¹ ã€å­¦ä¹ å°ç»„")
                print(" - é¢„æœŸROI: é«˜ (æ¯æŠ•å…¥1å…ƒï¼Œé¢„æœŸå›æŠ¥2.5å…ƒ)")
            elif 'ä¸­ç­‰è¡¨ç°' in segment:
                print(" - èµ„æºåˆ†é…: ä¸­ç­‰æŠ•å…¥ (35%æ€»èµ„æº)")
                print(" - æ”¯æŒæªæ–½: å·¥ä½œåŠã€åœ¨çº¿èµ„æºã€å®šæœŸåé¦ˆ")
                print(" - é¢„æœŸROI: ä¸­ç­‰ (æ¯æŠ•å…¥1å…ƒï¼Œé¢„æœŸå›æŠ¥1.8å…ƒ)")
            else:
                print(" - èµ„æºåˆ†é…: åŸºç¡€æŠ•å…¥ (25%æ€»èµ„æº)")
                print(" - æ”¯æŒæªæ–½: è‡ªä¸»å­¦ä¹ ææ–™ã€è¿›é˜¶è¯¾ç¨‹")
                print(" - é¢„æœŸROI: ç¨³å®š (æ¯æŠ•å…¥1å…ƒï¼Œé¢„æœŸå›æŠ¥1.3å…ƒ)")
        
    
    #3.ROIåˆ†æ
    
    print("\nROIåˆ†æ:")
   
    #å‡è®¾æ•°æ®
    
    interventions = ['ä¸ªæ€§åŒ–è¾…å¯¼', 'å­¦ä¹ å·¥ä½œåŠ', 'åœ¨çº¿èµ„æº', 'åŒä¼´å¯¼å¸ˆ']
    costs_per_student = [2000, 800, 300, 1500]  # å…ƒ/å­¦ç”Ÿ
    expected_improvements = [15, 8, 5, 10]  # é¢„æœŸæˆç»©æå‡ç™¾åˆ†æ¯”
    student_counts = [25, 40, 60, 20]  # å„æªæ–½ç›®æ ‡å­¦ç”Ÿæ•°
   
    roi_data = []
    for i, (intervention, cost, improvement, count) in enumerate(zip(interventions, costs_per_student, expected_improvements, student_counts)):
        total_cost = cost * count
       
        #å‡è®¾æ¯æé«˜1åˆ†ä»·å€¼500å…ƒ
        
        total_benefit = improvement * 500 * count
        roi = (total_benefit - total_cost) / total_cost if total_cost > 0 else 0
       
        roi_data.append({
           'å¹²é¢„æªæ–½': intervention,
           'äººå‡æˆæœ¬': f'Â¥{cost:,}',
           'ç›®æ ‡å­¦ç”Ÿæ•°': count,
           'æ€»æˆæœ¬': f'Â¥{total_cost:,}',
           'é¢„æœŸæåˆ†(%)': improvement,
           'æ€»æ•ˆç›Š': f'Â¥{total_benefit:,}',
           'æŠ•èµ„å›æŠ¥ç‡': f'{roi:.2%}'
       })
   
    roi_df = pd.DataFrame(roi_data)
    print(roi_df.to_string(index=False))
   
    #å¯è§†åŒ–ROIåˆ†æ
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
   
    #å·¦å›¾: æˆæœ¬æ•ˆç›Šå¯¹æ¯”
    
    x = range(len(interventions))
    width = 0.35
   
    costs = [c * n for c, n in zip(costs_per_student, student_counts)]
    benefits = [imp * 500 * n for imp, n in zip(expected_improvements, student_counts)]
   
    ax1.bar([i - width/2 for i in x], costs, width, label='æ€»æˆæœ¬', color=COLORS[0])
    ax1.bar([i + width/2 for i in x], benefits, width, label='æ€»æ•ˆç›Š', color=COLORS[1])
    ax1.set_xlabel('å¹²é¢„æªæ–½')
    ax1.set_ylabel('é‡‘é¢ (å…ƒ)')
    ax1.set_title('ä¸åŒå¹²é¢„æªæ–½çš„æˆæœ¬æ•ˆç›Šåˆ†æ')
    ax1.set_xticks(x)
    ax1.set_xticklabels(interventions, rotation=45, ha='right')
    ax1.legend()
   
    #å³å›¾: ROIå¯¹æ¯”
    
    rois = [(b - c) / c for b, c in zip(benefits, costs)]
    bars = ax2.bar(x, rois, color=COLORS[2:6])
    ax2.set_xlabel('å¹²é¢„æªæ–½')
    ax2.set_ylabel('æŠ•èµ„å›æŠ¥ç‡ (ROI)')
    ax2.set_title('ä¸åŒå¹²é¢„æªæ–½çš„æŠ•èµ„å›æŠ¥ç‡')
    ax2.set_xticks(x)
    ax2.set_xticklabels(interventions, rotation=45, ha='right')
   
    #æ·»åŠ æ•°å€¼æ ‡ç­¾
    
    for bar, roi in zip(bars, rois):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2, height, f'{roi:.1%}', ha='center', va='bottom')
   
    plt.tight_layout()
    plt.savefig('../visualizations/roi_analysis.png', dpi=300, bbox_inches='tight')
    print("\nROIåˆ†æå›¾è¡¨å·²ä¿å­˜")
   
    results['roi_analysis'] = roi_df
   
    return results

# ====== é¢å¤–æŒ‘æˆ˜ ======

def missions_bonus(df,previous_results):
    """
    é¢å¤–æŒ‘æˆ˜: å¦‚æœåªèƒ½å®æ–½ä¸€ä¸ªå¹²é¢„æªæ–½
    """
    
    print("\n" + "="*60)
    print("é¢å¤–æŒ‘æˆ˜: å¦‚æœåªèƒ½å®æ–½ä¸€ä¸ªå¹²é¢„æªæ–½")
    print("="*60)
    
    #åŸºäºä¹‹å‰çš„åˆ†æé€‰æ‹©æœ€ä½³å¹²é¢„æªæ–½
    
    if 'roi_analysis' in previous_results:
       roi_df = previous_results['roi_analysis']
       
       # æ‰¾åˆ°ROIæœ€é«˜çš„æªæ–½
       
       roi_values = []
       for roi_str in roi_df['æŠ•èµ„å›æŠ¥ç‡']:
           
           # ä»å­—ç¬¦ä¸²ä¸­æå–ROIå€¼
           
            roi_value = float(roi_str.strip('%')) / 100
            roi_values.append(roi_value)
            
       best_idx = np.argmax(roi_values)
       best_intervention = roi_df.iloc[best_idx]
       
       print(f"\næ¨è: {best_intervention['å¹²é¢„æªæ–½']}")
       print("\næ•°æ®æ”¯æŒ:")
       print(f"1.æœ€é«˜ROI:{best_intervention['æŠ•èµ„å›æŠ¥ç‡']}")
       print("2.æˆæœ¬æ•ˆç›Šæ¯”æœ€ä¼˜")
       print(f"3.é¢„æœŸæ•ˆæœ:å¹³å‡æåˆ†{best_intervention['é¢„æœŸæåˆ†(%)']}%")
        
       print("\nå®æ–½ç»†èŠ‚:")
       print(f"-ç›®æ ‡ç¾¤ä½“:{best_intervention['ç›®æ ‡å­¦ç”Ÿæ•°']} åå­¦ç”Ÿ")
       print(f"-æ€»æˆæœ¬:{best_intervention['æ€»æˆæœ¬']}")
       print(f"-é¢„æœŸæ€»æ•ˆç›Š:{best_intervention['æ€»æ•ˆç›Š']}")
        
       print("\nç†ç”±:")
       print("1.åŸºäºæ•°æ®é©±åŠ¨å†³ç­–:ROIåˆ†ææ˜¾ç¤ºæ­¤é¡¹æªæ–½å›æŠ¥ç‡æœ€é«˜")
       print("2.å¯æ‰©å±•æ€§:æ˜“äºå®æ–½å’Œæ¨å¹¿")
       print("3.ç›®æ ‡æ˜ç¡®:é’ˆå¯¹æœ€éœ€è¦å¸®åŠ©çš„å­¦ç”Ÿç¾¤ä½“")
       print("4. å¯æŒç»­æ€§:é•¿æœŸæ•ˆæœå’ŒçŸ¥è¯†è½¬ç§»")
        
       return best_intervention
    
    return None

# ==================== ç”Ÿæˆåˆ†ææŠ¥å‘Š ====================

def generate_report(df, results_q1, results_q2, results_q3, best_intervention):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    
    import os
    os.makedirs('../reports', exist_ok=True)
    
    report_content = f"""
# BIå­¦ç”Ÿæ•°æ®åˆ†ææŠ¥å‘Š

## é¡¹ç›®æ¦‚è¿°
- åˆ†ææ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
- æ•°æ®è§„æ¨¡: {df.shape[0]} åå­¦ç”Ÿ
- é«˜é£é™©å­¦ç”Ÿæ¯”ä¾‹: {results_q2.get('at_risk_percentage', 0):.1f}%

## 1. æ‹›ç”Ÿä¼˜åŒ–åˆ†æ
### ä¸»è¦å‘ç°
- å…¥å­¦è€ƒè¯•é¢„æµ‹èƒ½åŠ›: RÂ² = {results_q1.get('entryexam_r2', 0):.3f}
- æœ€é‡è¦çš„é¢„æµ‹ç‰¹å¾: {results_q1.get('feature_importance', pd.DataFrame()).iloc[0]['feature'] if 'feature_importance' in results_q1 and len(results_q1['feature_importance']) > 0 else 'N/A'}

### å»ºè®®
- ä¿ç•™å…¥å­¦è€ƒè¯•ä½œä¸ºä¸»è¦ç­›é€‰å·¥å…·
- ç»“åˆæ•™è‚²èƒŒæ™¯å’Œå­¦ä¹ æ—¶é•¿è¿›è¡Œç»¼åˆè¯„ä¼°
- å»ºç«‹é¢„æµ‹æ¨¡å‹ä¼˜åŒ–å½•å–å†³ç­–

## 2. è¯¾ç¨‹æ”¯æŒç­–ç•¥
### é«˜é£é™©å­¦ç”Ÿåˆ†æ
- é«˜é£é™©å­¦ç”Ÿæ•°é‡: {results_q2.get('at_risk_count', 0)} äºº
- é«˜é£é™©å­¦ç”Ÿæ¯”ä¾‹: {results_q2.get('at_risk_percentage', 0):.1f}%

### æ”¯æŒè®¡åˆ’
- ç›®æ ‡ç¾¤ä½“: å¹³å‡æˆç»©ä½äº60åˆ†çš„å­¦ç”Ÿ
- å…³é”®æªæ–½: ä¸ªæ€§åŒ–è¾…å¯¼ã€å­¦ä¹ å·¥ä½œåŠã€é¢å¤–èµ„æº
- é¢„æœŸæˆæœ: é«˜é£é™©å­¦ç”Ÿæˆç»©æå‡20%

## 3. èµ„æºåˆ†é…ä¸ROI
### å­¦ç”Ÿç»†åˆ†
å°†å­¦ç”Ÿåˆ†ä¸º{len(results_q3.get('clusters', pd.DataFrame()))}ä¸ªç¾¤ä½“ï¼Œå®æ–½å·®å¼‚åŒ–æ”¯æŒç­–ç•¥ã€‚

### ROIåˆ†æ
æœ€ä½³å¹²é¢„æªæ–½: {best_intervention['å¹²é¢„æªæ–½'] if best_intervention is not None else 'N/A'}
æŠ•èµ„å›æŠ¥ç‡: {best_intervention['æŠ•èµ„å›æŠ¥ç‡'] if best_intervention is not None else 'N/A'}

## 4. æ€»ç»“ä¸å»ºè®®
### ç«‹å³è¡ŒåŠ¨
1. å®æ–½{best_intervention['å¹²é¢„æªæ–½'] if best_intervention is not None else 'æ¨èæªæ–½'}ï¼Œé‡ç‚¹å…³æ³¨é«˜é£é™©å­¦ç”Ÿ
2. ä¼˜åŒ–å½•å–æµç¨‹ï¼Œç»“åˆå¤šç§è¯„ä¼°æŒ‡æ ‡
3. å»ºç«‹å­¦ç”Ÿè¡¨ç°ç›‘æ§ç³»ç»Ÿ

### é•¿æœŸè§„åˆ’
1. æŒç»­æ”¶é›†æ•°æ®ä¼˜åŒ–åˆ†ææ¨¡å‹
2. æ‰©å±•æ”¯æŒè®¡åˆ’è¦†ç›–æ›´å¤šå­¦ç”Ÿ
3. å»ºç«‹æ•°æ®é©±åŠ¨çš„æ•™è‚²å†³ç­–æ–‡åŒ–
"""
    report_path = '../reports/full_analysis_report.md'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"\nå®Œæ•´åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")

# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»å‡½æ•°"""
    
    print("="*60)
    print("BIå­¦ç”Ÿæ•°æ®åˆ†æé¡¹ç›®")
    print("="*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    
    import os
    os.makedirs('../visualizations', exist_ok=True)
    os.makedirs('../reports', exist_ok=True)
    
    # 1. åŠ è½½æ•°æ®
    
    df = load_data()
    if df is None:
        return
    
    # 2. æ¢ç´¢æ€§æ•°æ®åˆ†æ
    
    df = exploration_data_analysis(df)
    
    # 3. é—®é¢˜1: æ‹›ç”Ÿä¼˜åŒ–
    
    results_q1 = missions_recruit(df)
    
    # 4. é—®é¢˜2: è¯¾ç¨‹æ”¯æŒç­–ç•¥
    
    results_q2 = missions_support(df)
    
    # 5. é—®é¢˜3: èµ„æºåˆ†é…ä¸ROI
    
    results_q3 = missions_allocation(df)
    
    # 6. é¢å¤–æŒ‘æˆ˜
    
    best_intervention = missions_bonus(df, results_q3)
    
    # 7. ç”ŸæˆæŠ¥å‘Š
    
    generate_report(df, results_q1, results_q2, results_q3, best_intervention)
    
    print("\n" + "="*60)
    print("æ•°æ®åˆ†æå®Œæˆ!")
    print("="*60)
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    print(" - visualizations/ - æ‰€æœ‰åˆ†æå›¾è¡¨")
    print(" - reports/ - åˆ†ææŠ¥å‘Š")
    print("\nä¸‹ä¸€æ­¥å»ºè®®:")
    print("  1. æŸ¥çœ‹ç”Ÿæˆçš„å¯è§†åŒ–å›¾è¡¨")
    print("  2. é˜…è¯»åˆ†ææŠ¥å‘Š")

if __name__ == "__main__":
    main()
    
    
    
      
        
    
    


 
